{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiktionary Parser\n",
    "\n",
    "Please put the dump file in `data/wiki`.\n",
    "The latest dump file can be downloaded from this URL: https://dumps.wikimedia.org/dewiktionary/latest/dewiktionary-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mwxml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e472c8ca2294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmwxml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmwxml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mwxml'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parser with MediaWiki XML utility\n",
    "\n",
    "Installation: pip install mwxml\n",
    "Repo: https://github.com/mediawiki-utilities/python-mwxml\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import stat\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import mwxml\n",
    "from mwxml import Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIGURATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dewiktionary-latest-pages-articles.xml\n"
     ]
    }
   ],
   "source": [
    "##### DIRECTORIES ######\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "WIKI = os.path.join(DATA_DIR, \"wiki\")\n",
    "PREPROCESSED_FILE_DIR = os.path.join(DATA_DIR, \"preprocessed\")\n",
    "\n",
    "##### FILES #####\n",
    "\n",
    "#http://kaskade.dwds.de/gramophone/\n",
    "DE_WIKTIONARY_DATA = \"de-wiktionary.data.txt\" ### old file\n",
    "\n",
    "#Self-preprocessed wiktionary dump\n",
    "# dewiktionary-latest-pages-articles.xml\n",
    "DUMP_FILE = [file for file in os.listdir(os.path.join(\"data\", \"wiki\")) if file.lower().endswith(\".xml\")][0]\n",
    "print(DUMP_FILE)\n",
    "\n",
    "#This file stores the preprocessed pages from wiktioanry dump file\n",
    "PAGES_ALL_LANG_FILE = \"cleaned_pages_all_lang\" ### for txt output\n",
    "PAGES_ONLY_GERMAN = \"cleaned_pages_german\" ### for txt output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCARD = [\"ˈ\", \"ː\", \"ʔ\", \"̯\", \"̩\", \"ˌ\", \"͡ \"] ### Cleanes the phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse = False ### set this to True if you want to parse the dump file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiktionary Page object\n",
    "\n",
    "The `WiktPage` object defines a Wiktionary Page from the dump file. Pages are processed by the `mwxml` parser. For each page, the parser  parses following information:\n",
    "- Page id\n",
    "- Title\n",
    "- Revisions\n",
    "\n",
    "The field revision is a very long string containing all the page texts. This includes IPA information, as well.\n",
    "\n",
    "IPA information begins with **{{Sprache|**. The used symbols \"{{\" and the label \"Sprache\" may differ for other language dumps!\n",
    "\n",
    "The method `parse_wiktionary` is the main method used for parsing the whole wiktionary dump. In this method, WiktPage-objects are created and IPA information are extracted and saved in the `IPA` member. The program also stores the language (`lang`), because some items in the wiktionary may refer to foreign words. Some of these items may be empty or incomplete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WiktPage(object):\n",
    "    def __init__(self, page: Page):\n",
    "        self.id = page.id\n",
    "        self.title = page.title\n",
    "        self.revisions = [rev.text for rev in page]\n",
    "        self.lang = None\n",
    "        self.IPA = None\n",
    "        self.syllabication = None ### not tracked\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Page_id = {self.id}, IPA = {self.IPA}\"\n",
    "\n",
    "    def set_ipa(self, ipa_string):\n",
    "        self.IPA = ipa_string\n",
    "\n",
    "    def __dict__(self):\n",
    "        return {\"id\": self.id, \"title\": self.title, \"IPA\": self.IPA, \"lang\": self.lang}\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(text):\n",
    "    \"\"\"\n",
    "    Removes HTML code from a string, e.g. \"<!--Spezialfall\tNICHTlöschen-->\"\n",
    "    :param text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    text = TAG_RE.sub('', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_to_csv(page_list, out_file=\"german\"):\n",
    "    words = [page.title for page in page_list]\n",
    "    ipa_transcriptions = [page.IPA for page in page_list]\n",
    "    cleaned_transcriptions = [''.join(token for token in page.IPA if token not in DISCARD) for page in page_list]\n",
    "    \n",
    "    d = {\"words\": words, \"ipa\": ipa_transcriptions, \"clean_ipa\": cleaned_transcriptions}\n",
    "    \n",
    "    df = pd.DataFrame(d)\n",
    "    file_name = \"wiki_corpus_full.csv\" if out_file==\"all\" else \"wiki_corpus_de.csv\"\n",
    "    df.to_csv(os.path.join(PREPROCESSED_FILE_DIR, file_name), mode=\"w\", encoding=\"utf-8\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be used if output file should be a txt file!\n",
    "def persist_pages(pages_list, delete_content=True, out_file=PAGES_ALL_LANG_FILE):\n",
    "    if not os.path.isdir(os.path.join(\".\", PREPROCESSED_FILE_DIR)):\n",
    "        os.makedirs(os.path.join(\".\", PREPROCESSED_FILE_DIR))\n",
    "\n",
    "    file = os.path.join(\".\", PREPROCESSED_FILE_DIR, out_file)\n",
    "\n",
    "    if delete_content:\n",
    "        # Delete content\n",
    "        raw = open(file, \"w\")\n",
    "        raw.close()\n",
    "\n",
    "        if out_file == PAGES_ALL_LANG_FILE:\n",
    "            with open(file+\".txt\", 'a', encoding=\"utf-8\") as f:\n",
    "                print(\"File:\", file)\n",
    "                for i, page in enumerate(pages_list):\n",
    "                    cleaned_ipa = ''.join(token for token in page.IPA if token not in DISCARD)\n",
    "                    f.write(page.title + \"\\t\" + page.IPA + \"\\t\" + cleaned_ipa + \"\\t\" + page.lang + \"\\n\")\n",
    "        else:\n",
    "            with open(file+\".txt\", 'a', encoding=\"utf-8\") as f:\n",
    "                print(\"File:\", file)\n",
    "                for i, page in enumerate(pages_list):\n",
    "                    cleaned_ipa = ''.join(token for token in page.IPA if token not in DISCARD)\n",
    "                    f.write(page.title + \"\\t\" + page.IPA + \"\\t\" + cleaned_ipa + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pages_from_dump(dump_file):\n",
    "    \"\"\"\n",
    "    Reads all pages from a wikipedia dump file\n",
    "    :param dump_file: wiktionary file as xml\n",
    "    :return: a generator of WiktPage objects\n",
    "    \"\"\"\n",
    "    for i, page in enumerate(dump_file.pages):\n",
    "        wiktpage = WiktPage(page)\n",
    "        yield wiktpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main parsing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main method!\n",
    "def parse_wiktionary(csv=True):\n",
    "    #### directory configurations\n",
    "    sys_name = os.name.lower() \n",
    "    if \"windows\" in sys_name or \"nt\" in sys_name:\n",
    "        print(\"Running on a Windows machine.\")\n",
    "        os.chmod(\".\", stat.S_IWRITE) ## write problem on Windows\n",
    "    if not os.path.isdir(os.path.join(\".\", WIKI)):\n",
    "        print(\"Directory created!\")\n",
    "        os.makedirs(os.path.join(\".\", WIKI))\n",
    "\n",
    "    file = os.path.join(\".\", WIKI, DUMP_FILE)\n",
    "    dump = mwxml.Dump.from_file(file)\n",
    "    processed_pages = list(read_pages_from_dump(dump))\n",
    "\n",
    "    print(\"Selecting pages to keep...\")\n",
    "    valid_pages = []\n",
    "    revisions_none = []\n",
    "    for i, page in enumerate(processed_pages):\n",
    "        revisions = page.revisions  # all revisions are saved into a list\n",
    "        text = revisions[0]\n",
    "        if text is None:\n",
    "            revisions_none.append(page)\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                # Get language\n",
    "                language = text.split(\"({{Sprache|\")[1].split(\"}}\")[0]\n",
    "                page.lang = str(language).lower()\n",
    "            except IndexError:\n",
    "                page.lang = None\n",
    "                pass\n",
    "\n",
    "            # Retrieve IPA\n",
    "            text = remove_tags(str(text))\n",
    "            ipa = None\n",
    "            try:\n",
    "                # Given: {{Lautschrift|ˈsmr̩̂tan}}\n",
    "                # 1st: {{Lautschrift|, ˈsmr̩̂tan}}\n",
    "                # 2nd: ˈsmr̩̂tan, }} --> ˈsmr̩̂tan\n",
    "                ipa = text.split(\"{{Lautschrift|\")[1].split(\"}}\")[0]\n",
    "                page.set_ipa(ipa)\n",
    "                valid_pages.append(page)\n",
    "            except IndexError:\n",
    "                page.set_ipa(None)\n",
    "\n",
    "    print(\"Page cleaning done... \")\n",
    "\n",
    "    ## Keep only pages with ipa and language != None - 601290\n",
    "    valid_pages = [page for page in valid_pages if page.IPA and page.lang]\n",
    "    print(\"Total pages - all languages: %s\" % str(len(valid_pages)))\n",
    "    if not csv:\n",
    "        persist_pages(valid_pages, out_file=PAGES_ALL_LANG_FILE)\n",
    "    else:\n",
    "        persist_to_csv(valid_pages, out_file=\"all\")\n",
    "    ## Save  only german pages - 497265\n",
    "    valid_pages = [page for page in valid_pages if page.lang == \"german\" or page.lang == \"deutsch\"]\n",
    "\n",
    "    print(\"Total pages with 'German' language: %s\" % str(len(valid_pages)))\n",
    "    print(\"Total pages with 'None' revisions: %s\" % str(len(revisions_none)))\n",
    "    print(\"Persisting kept pages....\")\n",
    "    if not csv:\n",
    "        persist_pages(valid_pages, out_file=PAGES_ONLY_GERMAN)\n",
    "    else:\n",
    "        persist_to_csv(valid_pages, out_file=\"german\")\n",
    "    print(\"Pages persisted!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if parse:\n",
    "    print(\"Processing started...\")\n",
    "    start = time.time()\n",
    "    parse_wiktionary(csv=True)\n",
    "    end = time.time()\n",
    "    print(\"Total duration: {}\".format(convert(end-start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of toy datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method should be used to create toy sequences from a `csv`-file.\n",
    "\n",
    "Parameters:\n",
    "1. `length`: defines the sequence length\n",
    "2. `samples`: defines how many sample sequences should be created\n",
    "3. `file`: the path to the csv file\n",
    "4. `cols`: the name of the columns, where words should be retrieved from\n",
    "5. `wiktionary`: A flag indicating if words/phonemes are taken from a parsed dump wiktionary file or not (used for the file name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(a=5, version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DIRECTORIES ######\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "WIKI = os.path.join(DATA_DIR, \"wiki\")\n",
    "PREPROCESSED_FILE_DIR = os.path.join(DATA_DIR, \"preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file, cols,  gr=False, sep=\"\\t\"):\n",
    "    df = pd.read_csv(file, sep=sep)\n",
    "    print(df.columns)\n",
    "    #df1 = df[['a','b']]\n",
    "    df = df[[cols[0], cols[1]]].astype(str) \n",
    "    words = df[cols[0]].values\n",
    "    ipas = df[cols[1]].values\n",
    "    return words.tolist(), ipas.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_toy_dataset(length=3, samples=50000, file =\"data/phonemes-de-de.csv\", cols=[\"word_lc\", \"phonemes\"], wiktionary=True, sep=\"\\t\", \n",
    "                         only_same_type=\"p\"):\n",
    "    \n",
    "    src_matrix, target_matrix = [], []\n",
    "    words, ipa_transcriptions = get_data(file=file, cols=cols, sep=sep)\n",
    "    s = np.arange(len(words)) ## index array\n",
    "    \n",
    "    print(words[:10])\n",
    "    \n",
    "    if not samples:\n",
    "        samples = len(words) ## so many sequences as in the file\n",
    "    \n",
    "    random_idx = [[random.choice(s) for i in range(length)] for j in range(samples)]  ## pick indices\n",
    "    if only_same_type:\n",
    "            ### p == \"Phoneme\", \"g\" == \"grapheme\"\n",
    "        if only_same_type== \"p\":\n",
    "            ### this generates a corpus of ipa_words w/o spaces and their corresponding ipa_words w/ spaces\n",
    "            src_matrix = flatten([[''.join([ipa_transcriptions[i].lower() for i in seq])] for seq in random_idx]) ## word matrix    \n",
    "            target_matrix = flatten([[' '.join([ipa_transcriptions[i].lower() for i in seq])] for seq in random_idx]) ## ipa matrix\n",
    "        \n",
    "        if only_same_type== \"g\":\n",
    "            ### this generates a corpus of words w/o spaces and their corresponding words w/ spaces\n",
    "            src_matrix = flatten([[''.join([words[i].lower() for i in seq])] for seq in random_idx]) ## word matrix    \n",
    "            target_matrix = flatten([[' '.join([words[i].lower() for i in seq])] for seq in random_idx]) ## ipa matrix\n",
    "    else:\n",
    "        #### This generates a corpus of normal sequences, srcs are words and targets are ipa transcriptions\n",
    "        src_matrix = flatten([[' '.join([words[i].lower() for i in seq])] for seq in random_idx]) ## word matrix    \n",
    "        target_matrix = flatten([[' '.join([ipa_transcriptions[i].lower() for i in seq])] for seq in random_idx]) ## ipa matrix\n",
    "    \n",
    "   \n",
    "\n",
    "    import pandas as pd   \n",
    "    \n",
    "    if not wiktionary:\n",
    "        if only_same_type:\n",
    "            save_file = \"{}_toy_de_de{}.csv\".format(only_same_type, length)#\n",
    "        else: save_file = \"toy_de_de{}.csv\".format(length)#\n",
    "    else:\n",
    "        if only_same_type:\n",
    "             save_file = \"{}_toy_wiki_de-de{}.csv\".format(only_same_type, length)#\n",
    "        else: save_file = \"toy_wiki_de-de{}.csv\".format(length)#\n",
    "\n",
    "    df = pd.DataFrame(zip(src_matrix, target_matrix), columns=[\"input\", \"target\"])\n",
    "    df.to_csv(path_or_buf=\"data/{}\".format(save_file), encoding=\"utf-8\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['words', 'ipa', 'clean_ipa'], dtype='object')\n",
      "['Hallo', 'Subregnum', 'Subdivisio', 'Phylum', 'Superphylum', 'Subphylum', 'Subordo', 'Subgenus', 'Subspezies', 'Tribus']\n"
     ]
    }
   ],
   "source": [
    "file = \"wiki_corpus_de.csv\"\n",
    "generate_toy_dataset(3, None, file=os.path.join(PREPROCESSED_FILE_DIR, file), cols=[\"words\", \"clean_ipa\"], sep=\",\", only_same_type=\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
